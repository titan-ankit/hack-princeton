from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.documents.base import Blob
from langchain_core.tools import tool
from datetime import datetime, date
from collections import defaultdict

import requests

from pypdf import PdfReader
from uuid import uuid4
import zipfile
from tqdm import tqdm
import warnings

from pydantic import BaseModel
from typing import IO, Optional
from typing_extensions import List, TypedDict
import io

from dotenv import load_dotenv
import os
from pathlib import Path

from llm import llm, embeddings, image_parser, text_splitter, prompt

load_dotenv()

STORAGE_PATH = Path(
    os.getenv("STORAGE_PATH", str((Path(__file__).parent.parent / "uploads").resolve()))
).resolve()

# TODO: text splitting might be best done within textbook units

class Retrieval(TypedDict):
    question: str
    documents: List[Document]
    response: BaseModel | str


class RAGToolInput(BaseModel):
    question: str
    schema: Optional[BaseModel] = None
    date_range: Optional[List[str]] = None


class Storage:
    """Handles the storage and retrieval of document embeddings using FAISS & SQLAlchemy."""
    
    def __init__(self, path: str, from_path: bool = False):
        """Initialize the Storage class.

        Args:
            path (str): The path where the FAISS index file will be located.
            from_path (bool): If True, load the vector store from the specified path if it exists.
                              If False, will initialize a new vector store.
        """
        self.vector_store = None
        self.FAISS_INDEX_PATH = path

        # if a vector store already exists at path, and user specifies from_path, then load vector store from path rather than intializing a new one.
        if from_path:
            if os.path.exists(path):
                self.vector_store = FAISS.load_local(
                    path, embeddings, allow_dangerous_deserialization=True
                )
            else:
                warnings.warn(f"FAISS index file not found at {path}")

    def retrieve(self, query: str) -> List[Document]:
        """Retrieves documents from the vector store based on similarity to a query.

        Args:
            query (str): The query string to search for.

        Returns:
            List[Document]: A list of documents that match the query.
        """
        
        if not self.vector_store:
            raise ValueError("Vector store not initialized.")
        return self.vector_store.similarity_search(query)

    def rag(self, question: str, schema: Optional[BaseModel] = None, date_range: Optional[List[str]] = None):
        """Retrieve documents relevant to the input and generates a response. The documents here are state legislature records, including meeting transcripts, approved bills, and journals (daily notes of all legislature activities). Please use this to find information relevant to a given topic or issue. You can specify the date range of the outputs, to find more relevant information.

        Args:
            question (str): The question to retrieve context for.
            schema (Optional[BaseModel]): The schema for structured output. Defaults to None.
            date_range (Optional[List[str]]): A list containing the start and end date for filtering documents, in ["YYYY-MM-DD", "YYYY-MM-DD"] format.
        Returns:
            The response generated by the LLM based on the retrieved documents.
        """

        retrieved_docs = self.vector_store.similarity_search(question, k=15)

        # Filter by date range
        if date_range and len(date_range) == 2:
            start_date_str, end_date_str = date_range
            start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date() if start_date_str else None
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date() if end_date_str else None
            
            filtered_docs = []
            for doc in retrieved_docs:
                if 'journal_date' in doc.metadata and doc.metadata['journal_date']:
                    doc_date = doc.metadata['journal_date']
                    if isinstance(doc_date, str):
                        try:
                            doc_date = datetime.strptime(doc_date, "%Y-%m-%d").date()
                        except ValueError:
                            continue
                    
                    if (not start_date or (start_date <= doc_date)) and (not end_date or (doc_date <= end_date)):
                        filtered_docs.append(doc)
            retrieved_docs = filtered_docs
        
        docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)
        messages = prompt.invoke({"question": question, "context": docs_content})
        
        if schema:
            response = llm.with_structured_output(schema).invoke(messages)
        else:
            response = llm.invoke(messages).content
        
        return Retrieval(
            question=question,
            documents=retrieved_docs,
            response=response,
        )
        
    def add_documents(self, documents: List[Document]):
        """Adds documents to the vector store.
        
        Args:
            documents (List[Document]): A list of Document objects to be added.
        """
        if (
            not self.vector_store
        ):  # if vector store is not initialized, create a new one
            self.vector_store = FAISS.from_documents(
                documents=documents, embedding=embeddings
            )
        else:  # otherwise, add to existing vector store
            self.vector_store.add_documents(documents=documents)
        
        self.vector_store.save_local(self.FAISS_INDEX_PATH)

class PDF:
    def __init__(self, pdf_file: str | IO, storage: Storage, metadata: dict):
        """
        Initializes the Material class from PDF file.

        Args:
            pdf_file (str | IO): The path to the PDF file, or the file-like object, to load.
            storage (Storage): The Storage class for vector storage to associate with the material.
            metadata (dict): A dictionary containing metadata about the source file.
        """

        self.storage = storage
        document = self._load_documents(pdf_file)

        # Add the parsed metadata to the document itself.
        # This is useful if we want to see this info when retrieving docs.
        document.metadata.update(metadata)

        # split & store documents
        splits = text_splitter.split_documents([document])
        self.storage.add_documents(documents=splits)

    def _load_documents(self, pdf_file: str | IO) -> Document:
        """
        Loads documents from a PDF file.
        
        Args:
            pdf_file (str | IO): The path to the PDF file, or the file-like object, to load.
        
        Returns:
            Document: Document constructed from PDF content.
        """
        
        content = ""
        self.images = []
        
        reader = PdfReader(pdf_file)
        
        for page in tqdm(reader.pages, desc="Processing PDF pages"):
            content += page.extract_text() + "\n" # extract text from each page

        return Document(page_content=content, metadata={"source": pdf_file}, id=str(uuid4()))

def make_rag_tool(storage: Storage):
    @tool
    def rag(
        question: str,
        schema: Optional[BaseModel] = None,
        date_range: Optional[List[str]] = None,
    ):
        """Retrieve documents relevant to the input and generates a response. The documents here are state legislature records, including meeting transcripts, approved bills, and journals (daily notes of all legislature activities). Please use this to find information relevant to a given topic or issue. You can specify the date range of the outputs, to find more relevant information.

        Args:
            question (str): The question to retrieve context for.
            schema (Optional[BaseModel]): The schema for structured output. Defaults to None.
            date_range (Optional[List[str]]): A list containing the start and end date for filtering documents, in ["YYYY-MM-DD", "YYYY-MM-DD"] format.
        Returns:
            The response generated by the LLM based on the retrieved documents.
        """
        payload = RAGToolInput(
            question=question,
            schema=schema,
            date_range=date_range,
        )
        return storage.rag(**payload.dict())

    rag.__name__ = "rag"
    return rag
