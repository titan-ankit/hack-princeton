from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.documents.base import Blob

from sqlalchemy.orm import Session

import requests

from pypdf import PdfReader
from uuid import uuid4
import zipfile
from tqdm import tqdm
import warnings

from pydantic import BaseModel
from typing import Optional, IO
from typing_extensions import List, TypedDict
import io

from dotenv import load_dotenv
import os
from pathlib import Path

from llm import llm, embeddings, image_parser, text_splitter, prompt

load_dotenv()

STORAGE_PATH = Path(
    os.getenv("STORAGE_PATH", str((Path(__file__).parent.parent / "uploads").resolve()))
).resolve()

# TODO: text splitting might be best done within textbook units

class Retrieval(TypedDict):
    question: str
    documents: List[Document]
    response: BaseModel

class Storage:
    """Handles the storage and retrieval of document embeddings using FAISS & SQLAlchemy."""
    
    def __init__(self, path: str, database: Session, from_path: bool = False):
        """Initialize the Storage class.

        Args:
            path (str): The path where the FAISS index file will be located.
            from_path (bool): If True, load the vector store from the specified path if it exists.
                              If False, will initialize a new vector store.
        """
        self.vector_store = None
        self.FAISS_INDEX_PATH = path
        self.database = database

        # if a vector store already exists at path, and user specifies from_path, then load vector store from path rather than intializing a new one.
        if from_path:
            if os.path.exists(path):
                self.vector_store = FAISS.load_local(
                    path, embeddings, allow_dangerous_deserialization=True
                )
            else:
                warnings.warn(f"FAISS index file not found at {path}")

    def retrieve(self, query: str) -> List[Document]:
        """Retrieves documents from the vector store based on similarity to a query.

        Args:
            query (str): The query string to search for.

        Returns:
            List[Document]: A list of documents that match the query.
        """
        
        if not self.vector_store:
            raise ValueError("Vector store not initialized.")
        return self.vector_store.similarity_search(query)

    def rag(self, question: str, schema: Optional[BaseModel] = None):
        """Retrieve documents relevant to the input and generates a response.

        Args:
            question (str): The question to retrieve context for.
            schema (Optional[BaseModel]): The schema for structured output. Defaults to None.
        Returns:
            The response generated by the LLM based on the retrieved documents.
        """

        retrieved_docs = self.retrieve(question)
        docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)
        messages = prompt.invoke({"question": question, "context": docs_content})
        
        if schema:
            response = llm.with_structured_output(schema).invoke(messages)
        else:
            response = llm.invoke(messages).content
        
        return Retrieval(
            question=question,
            documents=retrieved_docs,
            response=response,
        )
        
    def add_documents(self, documents: List[Document], metadata: dict):
        """Adds documents to the vector store and records metadata in the database.
        
        Args:
            documents (List[Document]): A list of Document objects to be added.
            metadata (dict): A dictionary of metadata associated with the source file of the documents.
        """
        from db import ChunkMetadata # Import here to avoid circular dependency

        if (
            not self.vector_store
        ):  # if vector store is not initialized, create a new one
            self.vector_store = FAISS.from_documents(
                documents=documents, embedding=embeddings
            )
            # After initialization, the ids are in index_to_docstore_id
            ids = list(self.vector_store.index_to_docstore_id.values())
        else:  # otherwise, add to existing vector store
            ids = self.vector_store.add_documents(documents=documents)
        
        # Save metadata for each chunk
        for doc_id in ids:
            # Filter metadata to only include columns present in the ChunkMetadata table
            db_metadata = {k: v for k, v in metadata.items() if hasattr(ChunkMetadata, k)}
            chunk_record = ChunkMetadata(id=doc_id, **db_metadata)
            self.database.merge(chunk_record) # Use merge to handle potential re-runs

        self.database.commit()
        self.vector_store.save_local(self.FAISS_INDEX_PATH)

class PDF:
    def __init__(self, pdf_file: str | IO, storage: Storage, metadata: dict):
        """
        Initializes the Material class from PDF file.

        Args:
            pdf_file (str | IO): The path to the PDF file, or the file-like object, to load.
            storage (Storage): The Storage class for vector storage to associate with the material.
            metadata (dict): A dictionary containing metadata about the source file.
        """
        self._LOAD_METHOD = os.environ.get("LOAD_METHOD", "basic")

        self.storage = storage
        document = self._load_documents(pdf_file)

        # Add the parsed metadata to the document itself.
        # This is useful if we want to see this info when retrieving docs.
        document.metadata.update(metadata)

        # split & store documents
        splits = text_splitter.split_documents([document])
        self.storage.add_documents(documents=splits, metadata=metadata)

    def _load_documents(self, pdf_file: str | IO) -> Document:
        """
        Loads documents from a PDF file.
        
        Args:
            pdf_file (str | IO): The path to the PDF file, or the file-like object, to load.
        
        Returns:
            Document: Document constructed from PDF content.
        """
        
        content = ""
        self.images = []
        
        reader = PdfReader(pdf_file)
        
        for page in tqdm(reader.pages, desc="Processing PDF pages"):
            content += page.extract_text() + "\n" # extract text from each page

        return Document(page_content=content, metadata={"source": pdf_file}, id=str(uuid4()))